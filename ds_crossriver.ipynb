 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST_6OZKDOF-z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/content/crossriver.csv')"
   ],
   "metadata": {
    "id": "uUSep5rWdzC4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First Data exploration"
   ],
   "metadata": {
    "id": "QLDMyEzgRu45"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "ubaqTMsdH3p1",
    "outputId": "e810e61a-164f-4504-e128-3731cb9b665e"
   },
   "execution_count": null,
   "outputs": []
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "fYn8yzYadzGD",
    "outputId": "9f06f715-d305-4d5b-943f-7e723254341d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "id": "kziqLiAWiF1n",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    "outputId": "2a2b1f15-f01d-454b-cb16-2d454e572d03"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.nunique()"
   ],
   "metadata": {
    "id": "ObeAroWziF8F",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    "outputId": "dc49cdbe-587b-468a-9d97-125467ed18fd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.loan_status.mean()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "aQYpQkQ-fW_t",
    "outputId": "9536f192-2ad5-4a00-ccf8-7a3d3871eb9e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Feautre Engineering and selection"
   ],
   "metadata": {
    "id": "Ap6fMoGVOpEy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Variable descriptions, types, and suitability for loan application scoring:\n",
    "\n",
    "# Target Variable:\n",
    "# loan_status:  Indicates whether the loan was paid back or defaulted.  This is the target variable for predicting loan risk.  Crucial for application scoring.\n",
    "\n",
    "# Continuous Variables:\n",
    "# borrower_age:  Numeric representation of the borrower's age.  Suitable.\n",
    "# borrower_income:  The borrower's annual income.  Suitable.\n",
    "# borrower_emp_length:  Length of employment in years.  Suitable, though might need careful treatment of missing or extreme values.\n",
    "# loan_amnt:  The amount of the loan requested. Suitable.\n",
    "# loan_int_rate:  The interest rate on the loan.  Suitable.\n",
    "# LTI: Loan-to-income ratio.  Suitable and important for assessing risk.\n",
    "# cb_borrower_cred_hist_length: Length of the borrower's credit history. Suitable.\n",
    "\n",
    "# Discrete Variables:\n",
    "# borrower_home_ownership:  Type of home ownership (e.g., mortgage, rent, own). Suitable, needs encoding.\n",
    "# loan_purpose:  The reason for the loan (e.g., debt consolidation, home improvement).  Suitable, needs encoding.\n",
    "# loan_grade:  A categorical rating of the loan's risk.  Suitable, needs encoding (ordinal encoding might be best).\n",
    "# cb_borrower_default_on_file:  Indicates if the borrower has a history of defaults.  Suitable, needs encoding (binary).\n",
    "# borrower_homeownership_GRP:  (Likely a grouped version of borrower_home_ownership). Suitable, needs encoding.\n",
    "\n",
    "\n",
    "# Variable lists:\n",
    "\n",
    "target_variable = ['loan_status']\n",
    "\n",
    "continuous_variables = ['borrower_age', 'borrower_income', 'borrower_emp_length',\n",
    "                        'loan_amnt', 'loan_int_rate', 'LTI', 'cb_borrower_cred_hist_length']\n",
    "\n",
    "discrete_variables = ['borrower_home_ownership', 'loan_purpose', 'loan_grade',\n",
    "                      'cb_borrower_default_on_file']\n"
   ],
   "metadata": {
    "id": "DVAeZNl2LXYq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In credit risk modeling, proper dataset splitting ensures that the model is trained, tuned, and tested correctly to prevent data leakage and overfitting.\n",
    "\n",
    "1. Training Set (60%) → Model Learning\n",
    "The model learns patterns from borrower features and historical loan outcomes.\n",
    "Used to fit the model’s parameters (e.g., weights in logistic regression or decision tree splits).\n",
    "2. Validation Set (20%) → Model Tuning\n",
    "Used to optimize hyperparameters (e.g., regularization strength in logistic regression, number of trees in random forests).\n",
    "Helps in feature selection (removing redundant or highly correlated features).\n",
    "Prevents overfitting to training data by providing an independent dataset for evaluation before final testing.\n",
    "3. Test Set (20%) → Model Evaluation\n",
    "Used only once after model tuning to measure real-world performance.\n",
    "Ensures the model generalizes well to unseen data (new loan applications).\n",
    "Provides the final accuracy, precision-recall, AUC-ROC, and other key metrics to report to stakeholders."
   ],
   "metadata": {
    "id": "EaI4oyq5wcG7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable (loan status: approved, defaulted, etc.)\n",
    "y = df['loan_status']\n",
    "\n",
    "# Define the feature set (borrower and loan characteristics used for prediction)\n",
    "X = df[['borrower_age', 'borrower_income', 'borrower_home_ownership',\n",
    "        'borrower_emp_length', 'loan_purpose', 'loan_grade', 'loan_amnt',\n",
    "        'loan_int_rate', 'LTI', 'cb_borrower_default_on_file',\n",
    "        'cb_borrower_cred_hist_length']]\n",
    "\n",
    "# Step 1: Split into 60% Train and 40% Temp (Validation + Test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.4,   # 40% of data is reserved for Validation + Test\n",
    "    random_state=42,\n",
    "    stratify=y        # Ensures class distribution is preserved across sets\n",
    ")\n",
    "\n",
    "# Step 2: Split the remaining 40% into 20% Validation and 20% Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,    # Splitting 40% equally into 20% Validation and 20% Test\n",
    "    random_state=42,\n",
    "    stratify=y_temp   # Maintain class balance\n",
    ")\n",
    "\n",
    "# Final dataset distribution:\n",
    "# - Training: 60% (Used for model training)\n",
    "# - Validation: 20% (Used for hyperparameter tuning and feature selection)\n",
    "# - Test: 20% (Used for final evaluation and performance reporting)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_train['SPLIT'] = 'train'\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test['SPLIT'] = 'test'\n",
    "df_val = pd.concat([X_val, y_val], axis=1)\n",
    "df_val['SPLIT'] = 'val'\n",
    "df = pd.concat([df_train, df_test, df_val])"
   ],
   "metadata": {
    "id": "yZcw5wWuujQZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "NzV4wCdOwa3T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Auxilary functions\n",
    "def get_var_dist( var,df=df):\n",
    "  # Count the occurrences of each category\n",
    "  home_ownership_counts = df[var].value_counts()\n",
    "  # Create the bar chart\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  home_ownership_counts.plot(kind='bar')  # Use pandas plot directly\n",
    "  plt.title('Distribution of ' + var)\n",
    "  plt.xlabel(var)\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability if needed\n",
    "  plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def get_var_default(var,df=df):\n",
    "  grouped_means = df.groupby(var)['loan_status'].mean()\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  grouped_means.plot(kind='bar',color = 'red')\n",
    "  plt.title('Default rate by ' +var)\n",
    "  plt.xlabel(var)\n",
    "  plt.ylabel('Default rate')\n",
    "  plt.xticks(rotation=45, ha='right')\n",
    "  plt.show()\n",
    "\n",
    "  # prompt: df['loan_purpose_GRP'] = np.where(df['loan_purpose'].isin(['EDUCATION', 'VENTURE', 'PERSONAL']), 1, 2)\n",
    "# pls create a logistic regression model with only  this variable\n",
    "# and compute te roc auc score\n",
    "# pls take out the one ofthe category using drop fitrst\n",
    "# pls encapsulate in func and execute for loan_purpose_GRP\n",
    "\n",
    "\n",
    "def logistic_regression_model(df, variable):\n",
    "    # Create dummy variables using OneHotEncoder, dropping the first category\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first')\n",
    "    encoded_variable = enc.fit_transform(df[[variable]])\n",
    "\n",
    "    # Assuming 'loan_status' is your target variable\n",
    "    X = encoded_variable\n",
    "    y = df['loan_status']\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate the ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    print(f\"ROC AUC Score for {variable}: {roc_auc}\")\n",
    "\n",
    "def get_leaf_ranges(decision_tree, feature_names):\n",
    "    \"\"\"\n",
    "    Returns a dict of leaf_node -> { feature_name: (lower, upper) }\n",
    "    capturing the min/max constraints for each feature within that leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    tree_ = decision_tree.tree_\n",
    "    children_left = tree_.children_left\n",
    "    children_right = tree_.children_right\n",
    "    feature_index = tree_.feature\n",
    "    thresholds = tree_.threshold\n",
    "\n",
    "    # For convenience, map feature indices to actual feature names\n",
    "    idx_to_feature = {i: fn for i, fn in enumerate(feature_names)}\n",
    "\n",
    "    leaf_to_intervals = {}\n",
    "\n",
    "    def recurse(node_id, current_intervals):\n",
    "        \"\"\"\n",
    "        node_id: current node id\n",
    "        current_intervals: dict of feature_name -> (low_bound, high_bound)\n",
    "        \"\"\"\n",
    "        # If true, this node is a leaf\n",
    "        if children_left[node_id] == -1 and children_right[node_id] == -1:\n",
    "            leaf_to_intervals[node_id] = current_intervals\n",
    "            return\n",
    "\n",
    "        # Identify which feature is being split on, and the threshold\n",
    "        f_idx = feature_index[node_id]\n",
    "        thr = thresholds[node_id]\n",
    "        f_name = idx_to_feature[f_idx]\n",
    "\n",
    "        # Left child: X[f_name] <= thr\n",
    "        left_intervals = current_intervals.copy()\n",
    "        old_left_low, old_left_high = left_intervals[f_name]\n",
    "        # Update upper bound to min(old upper, thr)\n",
    "        new_left_high = min(old_left_high, thr)\n",
    "        left_intervals[f_name] = (old_left_low, new_left_high)\n",
    "\n",
    "        recurse(children_left[node_id], left_intervals)\n",
    "\n",
    "        # Right child: X[f_name] > thr\n",
    "        right_intervals = current_intervals.copy()\n",
    "        old_right_low, old_right_high = right_intervals[f_name]\n",
    "        # Update lower bound to max(old lower, thr)\n",
    "        new_right_low = max(old_right_low, thr)\n",
    "        right_intervals[f_name] = (new_right_low, old_right_high)\n",
    "\n",
    "        recurse(children_right[node_id], right_intervals)\n",
    "\n",
    "    # Initialize intervals for all features as (-inf, inf)\n",
    "    initial_intervals = {fn: (float('-inf'), float('inf')) for fn in feature_names}\n",
    "\n",
    "    # Start recursion at root node 0\n",
    "    recurse(0, initial_intervals)\n",
    "\n",
    "    return leaf_to_intervals\n",
    "def get_var_dist_percentiles(var, df):\n",
    "    \"\"\"\n",
    "    Displays the distribution of a variable using percentiles as a bar plot.\n",
    "    \"\"\"\n",
    "    percentiles = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    values = np.percentile(df[var].dropna(), percentiles)  # Handle potential NaN values\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([str(p) for p in percentiles], values, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {var} (Percentiles)')\n",
    "    plt.xlabel('Percentiles')\n",
    "    plt.ylabel(var)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def get_var_default_percentiles(var, df):\n",
    "    \"\"\"\n",
    "    Displays the default rate by percentiles of a variable using a bar plot.\n",
    "    \"\"\"\n",
    "    percentiles = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    df_sorted = df.sort_values(var)\n",
    "\n",
    "    default_rates = []\n",
    "    for p in percentiles:\n",
    "        idx = int(len(df_sorted) * p / 100)\n",
    "        default_rates.append(df_sorted['loan_status'][:idx].mean())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([str(p) for p in percentiles], default_rates, color='salmon', edgecolor='black')\n",
    "    plt.title(f'Default Rate by {var} Percentiles')\n",
    "    plt.xlabel('Percentiles of ' + var)\n",
    "    plt.ylabel('Default Rate')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage"
   ],
   "metadata": {
    "id": "ab9kaOubfgp3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "2#"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "BT6Rgxp0uTDh",
    "outputId": "1c611dcd-f142-41cc-c8b2-cefe8e221a70"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OILxWXeFuTGU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train[discrete_variables].nunique()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "oeNf_s_DKZGz",
    "outputId": "a7029141-7af9-4115-a84e-e5ce5f0d0473"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train[discrete_variables].info()\n",
    "#no nulls values to handle here."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "wWvdp85_OMN5",
    "outputId": "e88f1d02-8ec8-4db1-cabe-27f3b2a1c5eb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# We will first bin the discrete variables to reduce the number of categories and improve model performance.\n",
    "# This is especially important for categorical features with many unique values."
   ],
   "metadata": {
    "id": "hIU_KK9LNsuq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'borrower_home_ownership'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)\n",
    "#looks like 'other' does't have enough observations to be alone in a category\n",
    "#'other and 'rent has similar default rates so I'll bin them together."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rgl64P1PJUOk",
    "outputId": "5d2db7a1-052c-49f1-9824-1f5ce4efcf64"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Binning 'borrower_home_ownership' based on similar default rates\n",
    "# 'own' as 1, 'mortgage' as 2, and 'rent' and 'other' as 3\n",
    "df['borrower_homeownership_GRP'] = np.where(df['borrower_home_ownership'] == 'OWN', 1,\n",
    "                                            np.where(df['borrower_home_ownership'] == 'MORTGAGE', 2, 3))\n",
    "#The 'rent' and 'other' categories are combined into a single bin (3) because they show similar default rates."
   ],
   "metadata": {
    "id": "bu_d0N5QJY3-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Vesnr72iSdYJ",
    "outputId": "c2d41d10-49d9-48c4-a4c1-91d3cf4a86d5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(logistic_regression_model(df, 'borrower_homeownership_GRP'))\n",
    "#according to univariate results it looks like a strong variable"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "FaPrNnegSQko",
    "outputId": "de9e7f89-8554-4668-d0f7-88c5c9b9072a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'loan_purpose'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F4L7CTnzObCi",
    "outputId": "5490fdad-8eb3-408d-8db9-66de30d63293"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " df_train.groupby('loan_purpose')['loan_status'].mean()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "qV3n_4flSFmC",
    "outputId": "7577c7c7-6777-44a5-d775-94aae4e2fc9e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create separate groups for 'EDUCATION', 'VENTURE', 'PERSONAL', and others\n",
    "df['loan_purpose_GRP1'] = np.where(df['loan_purpose'] == 'EDUCATION', 2,\n",
    "                                 np.where(df['loan_purpose'] == 'VENTURE', 1,\n",
    "                                          np.where(df['loan_purpose'] == 'PERSONAL', 3, 4)))\n"
   ],
   "metadata": {
    "id": "OMji-sf2VI0R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(logistic_regression_model(df, 'loan_purpose_GRP'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "foF9meRgS7Y2",
    "outputId": "24668851-bd28-45f4-f005-e7fd52bb9987"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#other version of binning that we will check\n",
    "# Group 'EDUCATION', 'VENTURE', and 'PERSONAL' into group 1, others into group 2\n",
    "df['loan_purpose_GRP'] = np.where(df['loan_purpose'].isin(['EDUCATION', 'VENTURE', 'PERSONAL']), 1, 2)\n",
    "\n",
    "#The 'EDUCATION', 'VENTURE', and 'PERSONAL' loan purposes exhibit plus minus similar default rates,\n",
    "# Others loan purposes have also similar default rates and were grouped togethere as well."
   ],
   "metadata": {
    "id": "YhiU8jeMdYcN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(logistic_regression_model(df, 'loan_purpose_GRP'))\n",
    "#conclusion: according to univariate results it looks like that there is no purpose for the other version with more categories."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "eIGhh7yITDuB",
    "outputId": "4f59ecc7-61f2-401b-8009-298e162fbd0e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['cb_borrower_default_on_file'].replace({'Y': 1, 'N': 0}, inplace=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "NrZWk3Kre3T_",
    "outputId": "0f1857ea-858f-4c29-a363-d4dfadc1c3e4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'loan_grade'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)"
   ],
   "metadata": {
    "id": "Y_bJurM2iVZO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    "outputId": "f7390746-ada0-4f59-a45c-44e3f481be3b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " df_train.groupby('loan_grade').agg({'loan_status':['count','sum','mean']})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "m1vtll2_eeKf",
    "outputId": "4cabe8d2-eadd-4f57-cfff-433b682e48ac"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['loan_grade_GRP'] = np.where(df['loan_grade'] == 'A', 1,\n",
    "                                np.where(df['loan_grade'] == 'B', 2,\n",
    "                                         np.where(df['loan_grade'] == 'C', 3,\n",
    "                                                  np.where(df['loan_grade'].isin(['D', 'E', 'F']), 4,\n",
    "                                                           np.where(df['loan_grade'] == 'G', 5, 5)))))\n",
    "# #The loan grade 'G' has almost only defaults thus it was created as a seperate category\n",
    "# #altought it has only 64 observations."
   ],
   "metadata": {
    "id": "5JGrEL71iaP-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(logistic_regression_model(df, 'loan_grade_GRP'))\n",
    "#great variable"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "HhCifKosUBgj",
    "outputId": "1ffe08c0-1a93-49bd-ccd6-0368b4277830"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'cb_borrower_default_on_file'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DrXA8LXDecRp",
    "outputId": "8db2afb7-0481-486a-b910-eea391ac7b2c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#binning\n",
    "df['cb_borrower_default_on_file_GRP'] = df['cb_borrower_default_on_file'].replace({'Y': 1, 'N': 0})"
   ],
   "metadata": {
    "id": "qecuF-1ZiGCk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(logistic_regression_model(df, 'cb_borrower_default_on_file_GRP'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "WsxYh4lwUNlp",
    "outputId": "3289c1f2-206a-4085-f06f-6ca1f3dd40ad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.filter(like = 'GRP').columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "4KiLAaPRi2gU",
    "outputId": "ba4de366-8342-4bd6-aae9-8bd598d0ccda"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# now that discrete variables and nominal variables were binned ordinaly to the de defaul rate.\n",
    "# I'll quickly check multicolinearity with a spearman correlation.\n",
    "vars2check = ['borrower_homeownership_GRP', 'loan_purpose_GRP', 'loan_grade_GRP', 'cb_borrower_default_on_file_GRP'] + continuous_variables"
   ],
   "metadata": {
    "id": "xkE1YNjoi2jb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['SPLIT'] == 'train'][vars2check].corr(method = 'spearman').style.background_gradient(cmap = 'Blues')\n",
    "# I usually use CRAMER's V where i have only categorical varibales, here i have a mix of continuous and categorical variables so i thought that Spearman could be useful."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "OdWwjykTi2mP",
    "outputId": "0e0cb306-15b7-4f47-d39e-ce04ce30f680"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Takeaways**:\n",
    "\n",
    "\n",
    "Loan grade & interest rate (0.946): Almost a perfect linear relationship. Keeping both in a model could cause redundancy so I'll chose only one of the 2.\n",
    "Borrower age & credit history length (0.805): These variables convey almost the same information, I'll test an interaction variable here.\n",
    "Loan amount & LTI (0.650): Strong dependence—loan size significantly influences LTI could be also very interesting to create an interaction variable here using a decision tree.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "6Ibaz18xkk1L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_vif(df, features):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) for a given list of features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the dataset.\n",
    "    features (list): List of feature names to evaluate for multicollinearity.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with variables and their corresponding VIF scores.\n",
    "    \"\"\"\n",
    "    X = df[features].dropna()  # Drop NA to avoid errors\n",
    "    X = X.select_dtypes(include=[np.number])  # Ensure only numeric columns\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "vif_results = calculate_vif(df, vars2check)\n",
    "print(vif_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "xlR0QIofi2ot",
    "outputId": "cdd04c81-6e93-469d-e642-ef4905876ec2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Interpreting VIF Results:\n",
    "# VIF < 5 → Low multicollinearity (Good)\n",
    "# VIF 5-10 → Moderate multicollinearity (Consider removing)\n",
    "# VIF > 10 → High multicollinearity (Should be addressed)\n",
    "# key takeaway: as in the spearman correlation so we can see here that we have clear multicolinearity that should be adressed"
   ],
   "metadata": {
    "id": "mLpyB5PGi2r2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interpreting VIF Results:\n",
    "VIF < 5 → Low multicollinearity (Good)\n",
    "\n",
    "VIF 5-10 → Moderate multicollinearity (Consider removing)\n",
    "\n",
    "VIF > 10 → High multicollinearity (Should be addressed)\n",
    "\n",
    "key takeaway: as in the spearman correlation we can see here that we have clear multicolinearity that should be adressed."
   ],
   "metadata": {
    "id": "I57u_e8ioKUt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binning of continuous variables"
   ],
   "metadata": {
    "id": "h0PXu6AYohHy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "continuous_variables"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "SUl2v0obiGFw",
    "outputId": "d22eee9d-7374-4418-b97e-269cce7d5120"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'borrower_age'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wezXJnWqELXC",
    "outputId": "6f9bbac6-83a5-4f24-b31e-4edaba894cb0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'cb_borrower_cred_hist_length'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SbkXrNZ0EXwq",
    "outputId": "c6ceade1-33e8-4d8b-bd89-c7dde6dfe5be"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# by checking those 2 variables we can clearly see that those variables are almost not correlated at all with the target variable\n",
    "# even though it looks like we have some pattern of higher defaults in the left of the graphs. the density there is very low so we can't deduce anything.\n",
    "# conclusions I will omit those variables from the model."
   ],
   "metadata": {
    "id": "_J_kR1M7EXz_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GDh0Zzr0Fjb1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['SPLIT'] == 'train'][['borrower_income',\n",
    " 'borrower_emp_length',\n",
    " 'loan_amnt',\n",
    " 'loan_int_rate',\n",
    " 'LTI']].corr(method = 'spearman').style.background_gradient(cmap = 'Blues')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "c1b1TixaFjfU",
    "outputId": "94ff9381-a9c2-4b88-cdd4-7e6dd2be9fc8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interaction of LTI and loan_amnt"
   ],
   "metadata": {
    "id": "TdOO3Y7kBwQV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# PURPOSE: This code trains a decision tree on 'loan_amnt' and 'LTI'\n",
    "# to generate leaf-based segments (via leaf nodes). It then clusters these leaf nodes into three\n",
    "# categories using KMeans. Finally, it merges the resulting categories and leaf intervals back\n",
    "# into the original dataframe for further analysis.\n",
    "# this technique allows me to autotmate interaction variable binning.\n",
    "# i tried to find the minimal groups that will give a good predication of univariate model.\n",
    "# Define the variable names for age and credit history length\n",
    "var1 = 'LTI'\n",
    "var2 = 'loan_amnt'\n",
    "\n",
    "#define max leaf_nodes\n",
    "n = 12\n",
    "#grouping - define max categories (unify leafnodes using kmeans)\n",
    "g = 3\n",
    "\n",
    "# Prepare the features (X) and target variable (y) using only the training split\n",
    "X = df[df['SPLIT'] == 'train'][[var1, var2]]\n",
    "y = df[df['SPLIT'] == 'train']['loan_status']\n",
    "\n",
    "# Initialize and fit a decision tree model\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes=n, min_samples_leaf=500)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Plot the trained decision tree\n",
    "plt.figure(figsize=(15, 8))\n",
    "plot_tree(tree, feature_names=[var1, var2], filled=True, fontsize=7)\n",
    "plt.show()\n",
    "\n",
    "# Apply the trained tree to the entire dataset to get the leaf node for each row\n",
    "leaf_nodes_var_TREE = tree.apply(df[[var1, var2]])\n",
    "df['LEAF_NODE_' + 'interaction_loan_amnt_LTI' + '_tree'] = leaf_nodes_var_TREE\n",
    "\n",
    "# Group the training data by leaf node and compute count, sum of defaults, and default rate (dr)\n",
    "drs = (\n",
    "    df[df['SPLIT'] == 'train']\n",
    "    .groupby(leaf_nodes_var_TREE[df[df['SPLIT'] == 'train'].index])\n",
    "    .agg({'loan_status': ['count', 'sum', 'mean']})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns to meaningful labels\n",
    "drs.columns = [\n",
    "    'LEAF_NODE_' + 'interaction_loan_amnt_LTI' + '_tree',\n",
    "    'count',\n",
    "    'default',\n",
    "    'dr'\n",
    "]\n",
    "\n",
    "# Sort the leaf nodes by default rate in ascending order\n",
    "drs = drs.sort_values(by='dr', ascending=True)\n",
    "\n",
    "# Convert leaf nodes to an ordinal list for subsequent operations\n",
    "to_ord(list(drs['LEAF_NODE_' + 'interaction_loan_amnt_LTI' + '_tree']))\n",
    "\n",
    "# Cluster the default rates into g groups using KMeans\n",
    "kmeans = KMeans(n_clusters=g, random_state=42)\n",
    "leaf_node_clusters = kmeans.fit_predict(drs['dr'].values.reshape((-1, 1)))\n",
    "\n",
    "# Map the cluster labels to an ordinal category\n",
    "drs[var1 + '_' + var2+'_GRP'] = to_ord(leaf_node_clusters)\n",
    "\n",
    "# If the CAT_ column already exists in df, drop it to avoid conflicts before merging\n",
    "if var1 + '_' + var2+'_GRP' in df.columns:\n",
    "    df = df.drop(var1 + '_' + var2+'_GRP', axis=1)\n",
    "\n",
    "# Merge the cluster categories from 'drs' back into the main dataframe\n",
    "df = df.merge(\n",
    "    drs[[\n",
    "        'LEAF_NODE_' + 'interaction_loan_amnt_LTI' + '_tree',\n",
    "        var1 + '_' + var2+'_GRP'\n",
    "    ]],\n",
    "    on='LEAF_NODE_' + 'interaction_loan_amnt_LTI' + '_tree',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Extract leaf intervals from the trained tree for the two variables\n",
    "leaf_intervals = get_leaf_ranges(tree, [var1, var2])\n",
    "df_leaf_intervals = pd.DataFrame(leaf_intervals).T\n",
    "df_leaf_intervals.columns = ['Interval_' + var1, 'Interval_' + var2]\n",
    "print(logistic_regression_model(df, var1 + '_' + var2+'_GRP'))\n",
    "# Merge the leaf node default rate summary (drs) with the leaf intervals\n",
    "drs.merge(\n",
    "    df_leaf_intervals,\n",
    "    left_on='LEAF_NODE_interaction_loan_amnt_LTI_tree',\n",
    "    right_index=True\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o2wja4lFBmgV",
    "outputId": "eb546b45-b292-4a1d-c995-be8f50285b72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['SPLIT'] == 'train'][['borrower_income',\n",
    " 'borrower_emp_length',\n",
    " 'loan_int_rate',\n",
    " ]].nunique()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "FRwNKDE76TZ9",
    "outputId": "f46ee4a6-5131-421e-c1ae-f0945a683293"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'borrower_emp_length'\n",
    "get_var_dist(var,df_train)\n",
    "get_var_default(var,df_train)\n",
    "#this variable looks very weak, i'll omit it for now."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lQECzjQEK6Ax",
    "outputId": "2d15e85e-96c7-4b9b-c1e0-9dc513f7f14d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'loan_int_rate'\n",
    "get_var_dist_percentiles(var, df_train)\n",
    "get_var_default_percentiles(var, df_train)\n",
    "#although this variable looks very good for a model, it is highly correlated with loan_grade which looks better.\n",
    "#I'll then omit this variable to avoid multicolinearity - if i had more time i would of cours also binned it and check it as an alternative to loan_grade."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oSLtdEz8K6D_",
    "outputId": "fb58b6bb-7b04-4b1b-be84-4a2f989b4f76"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "var = 'borrower_income'\n",
    "get_var_dist_percentiles(var, df_train)\n",
    "get_var_default_percentiles(var, df_train)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Outw5_f_JpV-",
    "outputId": "d0855458-f86d-4d2c-9804-a7a8de298311"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#leaf_nodes\n",
    "n = 4\n",
    "#grouping\n",
    "g = 4\n",
    "def to_ord(my_list):\n",
    "    \"\"\"\n",
    "    Convert a list of values into an ordinal list of categories.\n",
    "    If two consecutive items in my_list are the same, they get the same category.\n",
    "    Otherwise, increment the category.\n",
    "    \"\"\"\n",
    "    ord_list = [1]\n",
    "    cat = 1\n",
    "    for current_val, next_val in zip(my_list[:-1], my_list[1:]):\n",
    "        if current_val == next_val:\n",
    "            ord_list.append(cat)\n",
    "        else:\n",
    "            cat += 1\n",
    "            ord_list.append(cat)\n",
    "    return ord_list\n",
    "\n",
    "# Assume df is your DataFrame, containing a column named 'split' with\n",
    "# values like 'train'/'test'/'oot', plus:\n",
    "#   - var (numeric feature)\n",
    "#   - 'loan_status' (the binary target, 0/1).\n",
    "# Adjust as needed for your actual DataFrame.\n",
    "\n",
    "var ='borrower_income'\n",
    "X = df.loc[df['SPLIT'] == 'train'][[var]]\n",
    "y = df.loc[df['SPLIT'] == 'train', 'loan_status']\n",
    "\n",
    "# 1) Train a decision tree with a maximum of n leaf nodes and\n",
    "#    minimum samples per leaf of 2000, then plot it.\n",
    "tree = DecisionTreeClassifier(max_leaf_nodes=n, min_samples_leaf=2000)\n",
    "tree.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plot_tree(tree, feature_names=[var], filled=True, fontsize=7)\n",
    "plt.show()\n",
    "\n",
    "# 2) Assign each observation to the tree’s leaf node\n",
    "leaf_nodes = tree.apply(df[var].values.reshape(-1, 1))\n",
    "df[f'LEAF_NODE_{var}_tree'] = leaf_nodes\n",
    "\n",
    "# 3) For each leaf node, calculate count, sum of defaults, and default rate\n",
    "drs = (\n",
    "    df\n",
    "    .groupby(f'LEAF_NODE_{var}_tree')['loan_status']\n",
    "    .agg(['count', 'sum', 'mean'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'sum': 'default', 'mean': 'dr'})\n",
    "    .sort_values(by='dr', ascending=True)\n",
    ")\n",
    "\n",
    "# 4) Use KMeans to cluster the leaf nodes by their default rate\n",
    "kmeans = KMeans(n_clusters=g, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(drs['dr'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert clusters into ordinal categories\n",
    "drs[var + '_GRP'] = to_ord(cluster_labels)\n",
    "\n",
    "# 5) Merge the new categories back onto the main df\n",
    "# First remove old column if it exists\n",
    "cat_col_name = var + '_GRP'\n",
    "if cat_col_name in df.columns:\n",
    "    df.drop(columns=cat_col_name, inplace=True)\n",
    "\n",
    "df = df.merge(\n",
    "    drs[[f'LEAF_NODE_{var}_tree', cat_col_name]],\n",
    "    on=f'LEAF_NODE_{var}_tree',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the final default-rate summary table\n",
    "\n",
    "leaf_intervals = get_leaf_ranges(tree, [var])\n",
    "df_leaf_intervals = pd.DataFrame(leaf_intervals).T\n",
    "df_leaf_intervals.columns = ['Interval_' + var]\n",
    "print(logistic_regression_model(df, var + '_GRP'))\n",
    "# Merge the leaf node default rate summary (drs) with the leaf intervals\n",
    "drs.merge(\n",
    "    df_leaf_intervals,\n",
    "    left_on=f'LEAF_NODE_{var}_tree',\n",
    "    right_index=True\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "id": "Yjc_jOq0oZ7i",
    "outputId": "5159e555-878b-4422-ef08-c304f8cce9ec"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#the result above keeps ordinality , so logically as someone earns more money as it has less defaults."
   ],
   "metadata": {
    "id": "DWwZB2FIRgk_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#drop grp1\n",
    "df.drop(columns='loan_purpose_GRP1', inplace=True)\n"
   ],
   "metadata": {
    "id": "cg5uKliXVNM3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.filter(like = 'GRP').columns\n",
    "# so as we can see we have 6 variables\n",
    "# the variable LTI_loan_amnt_GRP is an interaction of 2 variables."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "xcQeRuWGR2to",
    "outputId": "be3a4bbb-99d9-4a1d-99b5-07b4a8883e9d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cat_var = ['borrower_homeownership_GRP', 'loan_purpose_GRP', 'loan_grade_GRP',\n",
    "       'cb_borrower_default_on_file_GRP', 'LTI_loan_amnt_GRP',\n",
    "       'borrower_income_GRP']"
   ],
   "metadata": {
    "id": "yIXwunfmR2xU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df[df['SPLIT'] == 'train'][cat_var].corr(method = 'spearman').style.background_gradient(cmap = 'Blues')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FrKxD8Z3R203",
    "outputId": "8edf8594-2c62-4e36-8c1d-81b2e9ecc1d6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vif_results = calculate_vif(df, cat_var)\n",
    "print(vif_results)\n",
    "# we still have some multi colinearity issue between loan grade and cb_borrower_default_on_file_GRP\n",
    "# let\"s try to run a model and have a check on impact\n",
    "# we can also create an intearction variable for those 2 later"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "k5-jRtsKR24D",
    "outputId": "6ac2dcf0-c478-4417-ab29-77e92bb9f1a6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[cat_var + ['SPLIT' , 'loan_status'] ]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ZDR8aiCCR27M",
    "outputId": "88d12fbf-c6c0-4fee-c509-053952d4e3e3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(ridge_logit_final.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "dgdPp6Q5aEq8",
    "outputId": "0fa6126f-b0e6-4d16-cedb-4d199b21c4db"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YqYYSAPzaEuu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SvH24zX_aEyQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define categorical variables\n",
    "cat_var = ['borrower_homeownership_GRP', 'loan_purpose_GRP', 'loan_grade_GRP',\n",
    "           'cb_borrower_default_on_file_GRP', 'LTI_loan_amnt_GRP',\n",
    "           'borrower_income_GRP']\n",
    "\n",
    "# Separate train and test datasets\n",
    "df_train = df[df['SPLIT'] == 'train']\n",
    "df_test = df[df['SPLIT'] == 'test']\n",
    "\n",
    "X_train = df_train[cat_var]\n",
    "y_train = df_train['loan_status']\n",
    "\n",
    "X_test = df_test[cat_var]\n",
    "y_test = df_test['loan_status']\n",
    "\n",
    "# Standardize features (not always necessary for categorical variables, but helps with regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform Ridge-regularized logistic regression to limit the number of variables\n",
    "ridge_logit = LogisticRegression(penalty='l2', C=0.1, solver='liblinear')  # C is the inverse of regularization strength\n",
    "ridge_logit.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the coefficients and select the top 6 most important features\n",
    "coefs = pd.Series(ridge_logit.coef_[0], index=cat_var).abs().sort_values(ascending=False)\n",
    "selected_vars = coefs[:6].index.tolist()  # Select the top 6 variables\n",
    "\n",
    "print(f\"Selected Variables: {selected_vars}\")\n",
    "\n",
    "# Refit the logistic regression model with only the selected variables\n",
    "X_train_selected = df_train[selected_vars]\n",
    "X_test_selected = df_test[selected_vars]\n",
    "\n",
    "ridge_logit_final = LogisticRegression(penalty='l2', C=0.1, solver='liblinear')\n",
    "ridge_logit_final.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = ridge_logit_final.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "NpA4KpnlZwda",
    "outputId": "6571a030-d811-407f-9c9c-8293092a6a66"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define categorical variables\n",
    "cat_var = ['borrower_homeownership_GRP', 'loan_purpose_GRP', 'loan_grade_GRP',\n",
    "           'cb_borrower_default_on_file_GRP', 'LTI_loan_amnt_GRP',\n",
    "           'borrower_income_GRP']\n",
    "\n",
    "# Separate train and test datasets\n",
    "df_train = df[df['SPLIT'] == 'train']\n",
    "df_test = df[df['SPLIT'] == 'test']\n",
    "\n",
    "X_test = df_test[cat_var]\n",
    "y_test = df_test['loan_status']\n",
    "\n",
    "# Create formula string for the GLM model (Generalized Linear Model)\n",
    "formula = 'loan_status ~ ' + ' + '.join(cat_var)\n",
    "\n",
    "# Fit the logistic regression model using GLM with Binomial family\n",
    "model = smf.glm(formula, data=df_train, family=sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\n===== Model Summary (GLM - Binomial) =====\")\n",
    "print(model.summary())\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"\\nROC AUC Score: {roc_auc}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "oTSK9vSAR2-V",
    "outputId": "d56d9106-22a1-490c-9a70-13aedeff512b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(model.summary())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "TEs7eHSqaJuw",
    "outputId": "23067b4d-65ca-42b3-e4f0-0bcbefdac23a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(model.summary2())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    "id": "Q2kHGEssadcR",
    "outputId": "0f2ff579-f039-44d7-9c0c-974d2f5801f8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "p125FZZ8R03W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "zWyqUEIVR06c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xcLO01cNR09_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "53wspFVFR1SI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6xg2g4PbR1VL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract leaf intervals from the trained tree for the two variables\n",
    "leaf_intervals = get_leaf_ranges(tree, [var])\n",
    "df_leaf_intervals = pd.DataFrame(leaf_intervals).T\n",
    "df_leaf_intervals.columns = ['Interval_' + var]\n",
    "print(logistic_regression_model(df, var + '_GRP'))\n",
    "# Merge the leaf node default rate summary (drs) with the leaf intervals\n",
    "drs.merge(\n",
    "    df_leaf_intervals,\n",
    "    left_on='LEAF_NODE_interaction_loan_amnt_LTI_tree',\n",
    "    right_index=True\n",
    ")\n"
   ],
   "metadata": {
    "id": "Zxsz76R4NvBX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "k-plyfgZNvEN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract leaf intervals from the trained tree for the two variables\n",
    "leaf_intervals = get_leaf_ranges(tree, [var1, var2])\n",
    "df_leaf_intervals = pd.DataFrame(leaf_intervals).T\n",
    "df_leaf_intervals.columns = ['Interval_' + var1, 'Interval_' + var2]\n",
    "print(logistic_regression_model(df, var1 + '_' + var2+'_GRP'))\n",
    "# Merge the leaf node default rate summary (drs) with the leaf intervals\n",
    "drs.merge(\n",
    "    df_leaf_intervals,\n",
    "    left_on='LEAF_NODE_interaction_loan_amnt_LTI_tree',\n",
    "    right_index=True\n",
    ")\n"
   ],
   "metadata": {
    "id": "9EN5c1SSNlPD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_KPUep1ZoZ-P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YKDiZOlRoaBT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "udqHWxwUoaFR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "RQq3IyQ7oaMG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "id": "G3FVmiiygogK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xFWwfI0iicvW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "columns_to_encode = ['borrower_home_ownership','loan_purpose','loan_grade']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    # If it's an object or if you simply want to encode everything as categories:\n",
    "    if df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    # If you truly want to label-encode numeric columns as well, you can do so:\n",
    "    # else:\n",
    "    #     le = LabelEncoder()\n",
    "    #     df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n"
   ],
   "metadata": {
    "id": "wOZdj_i4jbSF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y = df['loan_status']\n",
    "X = df[['borrower_age', 'borrower_income', 'borrower_home_ownership',\n",
    "       'borrower_emp_length', 'loan_purpose', 'loan_grade', 'loan_amnt',\n",
    "       'loan_int_rate', 'LTI', 'cb_borrower_default_on_file',\n",
    "       'cb_borrower_cred_hist_length']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "metadata": {
    "id": "e2eeT2KqjbVJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.corr(method = 'spearman').style.background_gradient(cmap = 'Blues')"
   ],
   "metadata": {
    "id": "Skm03iJllUae"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "set(feature_names)"
   ],
   "metadata": {
    "id": "hhP6o9qAl5vI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "set(feature_names).symmetric_difference(set(X_train.columns))"
   ],
   "metadata": {
    "id": "lRhdOCKDlUgA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rf = RandomForestClassifier(random_state=42,n_estimators = 100, max_leaf_nodes = 12)"
   ],
   "metadata": {
    "id": "xCvJSyetjbYh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rfecv = RFECV(\n",
    "    estimator=rf,\n",
    "    step=1,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)"
   ],
   "metadata": {
    "id": "HSSELzRHj8Be"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_prob = rfecv.predict_proba(X_test)[:, 1]\n"
   ],
   "metadata": {
    "id": "rLuDeIcTkqkV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the ROC AUC Curve\n",
    "# -----------------------------------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "FD-_woFWj8Eb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cNar5fVkkpNl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#  Print RFECV results\n",
    "\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "print(\"Selected features: \", list(X_train.columns[rfecv.support_]))\n",
    "\n",
    "importances = rfecv.estimator_.feature_importances_\n",
    "feature_names = X_train.columns[rfecv.support_]  # Only the selected features\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances (Selected Features):\")\n",
    "print(importance_df)\n",
    "\n",
    "# Optional: plot the importances\n",
    "plt.figure(figsize=(8,6))\n",
    "importance_df.set_index('feature').plot(kind='bar')\n",
    "plt.title(\"Feature Importances (RFECV-Selected Features)\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "7-POqwSxkHTH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9sI76qtEkHeQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "7Hni2Bqzj8HK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "oHQ7flzHj8Jc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X.info()"
   ],
   "metadata": {
    "id": "t-ygi_7PixTH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.borrower_home_ownership.value_counts()\n",
    "df.nunique()"
   ],
   "metadata": {
    "id": "ISjxRzDQi5GW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for var in ['borrower_age', 'borrower_income', 'borrower_home_ownership',\n",
    "       'borrower_emp_length', 'loan_purpose', 'loan_grade', 'loan_amnt',\n",
    "       'loan_int_rate', 'LTI', 'cb_borrower_default_on_file',\n",
    "       'cb_borrower_cred_hist_length']:\n",
    "  get_var_dist(var)\n",
    "  get_var_default(var)"
   ],
   "metadata": {
    "id": "5wlR679TgiBP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Important Note\n",
    "# This dataset only includes loans that were *approved* by the lender.\n",
    "# However, many loan applicants are rejected, and we do not have data\n",
    "# on those individuals. This introduces **selection bias**, as the model\n",
    "# only learns patterns from accepted customers and may not generalize\n",
    "# well to new applicants.\n",
    "#\n",
    "# To correct this, several augmentation techniques exist:\n",
    "# - **Synthetic data generation** (e.g., SMOTE for underrepresented groups)\n",
    "# - **Reject Inference** (using proxy labels for rejected applicants)\n",
    "# - **Domain adaptation** (applying external datasets to correct bias)\n",
    "#\n",
    "# In this case, due to time constraints, **no augmentation techniques were applied**.\n",
    "# This means the model's predictions may not be valid for rejected applicants."
   ],
   "metadata": {
    "id": "VU5m-mCAiIDm"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
